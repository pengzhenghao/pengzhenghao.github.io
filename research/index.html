<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Research Statement | Zhenghao Peng </title> <meta name="author" content="Zhenghao Peng"> <meta name="description" content="Zhenghao Peng, Aug 18, 2025"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%87&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pengzhenghao.github.io/research/"> <script src="/assets/js/theme.js?190f555c772200f91d44820159d84552"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Zhenghao Peng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/research/">Research Statement <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV (PDF) </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Research Statement</h1> <p class="post-description">Zhenghao Peng, Aug 18, 2025</p> </header> <article> <style>.float-right-40{float:right;width:40%;margin-left:1.5em;margin-bottom:.5em}.float-right-40 figure{width:100%;margin:0}.float-right-40 img{width:100%;height:auto;display:block}.float-right-30{float:right;width:30%;margin-left:1.5em;margin-bottom:.5em}.float-right-30 figure{width:100%;margin:0}.float-right-30 img{width:100%;height:auto;display:block}.float-right-50{float:right;width:50%;margin-left:1em;margin-bottom:.5em}.float-right-50 figure{width:100%;margin:0}.float-right-50 img{width:100%;height:auto;display:block}.float-center{margin:0 auto;text-align:center}</style> <figure> <picture> <img src="/assets/img/RS-Intro.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>I like playing with real robots!</b></figcaption> </figure> <h3 id="road-to-physical-ai">Road to Physical AI</h3> <p>My research aims to build foundation models for robotics that are scalable, aligned, and deployable in the real world.</p> <p>As shown in the figure below, I organize this agenda into three complementary stages:</p> <ol> <li>Imitation learning pretraining,</li> <li>Closed-loop post-training, and</li> <li>Human-in-the-loop test-time adaptation.</li> </ol> <p>I started my research in 2018. Since then, I have published 16 papers in top-tier AI and robotics conferences, such as NeurIPS, ICLR, CoRL, ICRA, ICML, ECCV, <em>etc</em>. This research statement provides an overview of my work and future directions in each stage.</p> <figure> <picture> <img src="/assets/img/Framework.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Road to Physical AI:</b> The capacities of physical AI agent are acquired via pretraining, closed-loop finetuning, and human-in-the-loop test-time adaptation. Important problems in each stage are listed. ‚úÖ: I have experience on. üîç: Discovering.</figcaption> </figure> <p><br></p> <h3 id="imitation-learning-pretraining">Imitation Learning Pretraining</h3> <p>Imitation learning provides the foundation for physical AI. My recent work focuses on Vision-Language-Action (VLA) pretraining to endow models with embodied understanding and action‚Äìlanguage alignment.</p> <p>In <a href="https://metadriverse.github.io/metavqa/" rel="external nofollow noopener" target="_blank">MetaVQA</a>, we constructed datasets to provide 3D grounding and physical reasoning to VLAs.</p> <p>Currently, I am exploring methods to ensure the model understands the actions it produced semantically, enabling reasoning about both execution and consequences.</p> <p><br></p> <h3 id="closed-loop-post-training-simulation-and-rl">Closed-loop Post-Training: Simulation and RL</h3> <div class="float-right-30"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/RS-Simulation-480.webp 480w,/assets/img/RS-Simulation-800.webp 800w,/assets/img/RS-Simulation-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/RS-Simulation.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>While imitation learning offers strong initialization, open-loop behavior cloning often fails in real deployments. Closed-loop post-training in simulation bridges this gap. I‚Äôve been exploring the methods to improve the fidelity of data-driven simulation.</p> <p><strong>Simulation Platforms:</strong> I built <a href="https://github.com/metadriverse/metadrive" rel="external nofollow noopener" target="_blank">MetaDrive</a> and <a href="https://metadriverse.github.io/scenarionet/" rel="external nofollow noopener" target="_blank">ScenarioNet</a>, enabling large-scale data-driven simulation. MetaDrive has received 1,000+ GitHub stars and 350+ citations, becoming a widely used benchmark in the community.</p> <p><strong>Data-driven Environment Generation:</strong> <a href="https://arxiv.org/pdf/2506.23316" rel="external nofollow noopener" target="_blank">InfGen</a> is a multi-agent behavior model that augments dynamic multi-agent traffic situations. <a href="https://metadriverse.github.io/cat/" rel="external nofollow noopener" target="_blank">CAT</a> and <a href="https://arxiv.org/pdf/2506.09485" rel="external nofollow noopener" target="_blank">Adv-BMT</a> generate adversarial driving scenarios. We show that training RL agents on the scenarios generated by above models can greatly improve it‚Äôs closed-loop performance, showing that improving the dynamic realism of simulation is crucial for closed-loop RL.</p> <p><strong>Improving Visual Realism:</strong> <a href="https://metadriverse.github.io/simgen/" rel="external nofollow noopener" target="_blank">SceneGen</a> (Diffusion models) and <a href="https://metadriverse.github.io/vid2sim/" rel="external nofollow noopener" target="_blank">Vid2Sim</a> (3D Gaussian Splatting) rerender visual environments from real videos, creating photorealistic RL environments.</p> <p><strong>Closed-loop finetuning:</strong> We can post-train a large transformer model with <a href="https://arxiv.org/pdf/2409.18343" rel="external nofollow noopener" target="_blank">closed-loop RL finetuning</a>, which greatly improves the multi-agent behaviors.</p> <p>I am exploring the embodied reasoning as a new way to improve VLA.</p> <p><br></p> <h3 id="human-in-the-loop-test-time-adaptation">Human-in-the-Loop Test-Time Adaptation</h3> <div class="float-right-30"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/RS-Experiment-480.webp 480w,/assets/img/RS-Experiment-800.webp 800w,/assets/img/RS-Experiment-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/RS-Experiment.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Our method exhibits unprecedented learning efficiency.</figcaption> </figure> </div> <p>Even with large-scale simulation and RL, agents inevitably face a sim-to-real gap when deployed. Closing this gap requires humans in the loop, providing real-time interventions and feedback to align the agent‚Äôs behavior with human values.</p> <p>Over the years, I have pioneered this research direction with 5 papers:</p> <p><strong><a href="https://decisionforce.github.io/EGPO/" rel="external nofollow noopener" target="_blank">EGPO</a> (CoRL 2021):</strong> Our research on human-in-the-loop policy learning began in 2021. The first published work is Expert Guided Policy Optimization (EGPO). In this work, we explored how an RL agent can benefit from the intervention of a PPO expert.</p> <p><strong><a href="https://decisionforce.github.io/HACO/" rel="external nofollow noopener" target="_blank">HACO</a> (ICLR 2022):</strong> Building upon the methodology of EGPO, and substituting the PPO expert with a <strong>real human subject</strong>, we proposed Human-AI Copilot Optimization (HACO) and it demonstrated significant improvements in learning efficiency over traditional RL baselines.</p> <p><strong><a href="https://metadriverse.github.io/TS2C/" rel="external nofollow noopener" target="_blank">TS2C</a> (ICLR 2023):</strong> In Teacher-Student Shared Control (TS2C), we examined the impact of using the value function as a criterion for determining when the PPO expert should intervene. The value function-based intervention makes it possible for the student agent to learn from a suboptimal teacher.</p> <p><strong><a href="https://metadriverse.github.io/pvp/" rel="external nofollow noopener" target="_blank">Proxy Value Propagation (PVP)</a> (NeurIPS 2023 Spotlight):</strong> Considering the <strong>reward-free setting</strong>, we proposed several improvements to enhance learning from active human involvement. These improvements address issues observed in HACO, including the jittering and oscillation of the learning agent, catastrophic forgetting, and challenges in learning sparse yet crucial behaviors.</p> <p><strong><a href="https://metadriverse.github.io/pvp4real/" rel="external nofollow noopener" target="_blank">PVP4Real</a> (ICRA 2025):</strong> Deploying PVP on real robots, <strong>we train two mobile robots (Go2 and delivery robot) from scratch, without reward, in real world, in real time</strong>. We show that the embodied agents can learn (1) safe navigation and (2) human following tasks within 15 minutes on wall time.</p> <div class="float-center"> <figure> <video src="/assets/teaser/cover_pvp4real_compressed.mp4" class="img-fluid rounded" width="70%" height="auto" autoplay="" loop="" muted=""></video> <figcaption class="caption">We train real robots in 15 minutes with <a href="https://metadriverse.github.io/pvp4real/" rel="external nofollow noopener" target="_blank">human-in-the-loop learning</a>.</figcaption> </figure> </div> <p>This line of work establishes test-time post-training as the final stage of robotic learning: a continual feedback loop where human guidance ensures safety, adaptability, and alignment.</p> <p>One interesting direction I‚Äôve explored is <strong>assistive AI</strong>. Collaborating with Prof. Jonathan Kao, we have built <a href="https://www.arxiv.org/pdf/2409.15317" rel="external nofollow noopener" target="_blank">Interventional Diffusion Assistance (IDA)</a>, a novel AI framework for dynamically sharing control between human users and AI agents, empowering the human users to achieve better performance in control tasks via the synergy of human and AI assistant. Beyond assistive technology, this direction also holds promise for domains like rehabilitation, prosthetic control, and even high-stakes human‚Äìrobot collaboration in healthcare or industrial settings.</p> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> ¬© Copyright 2025 Zhenghao Peng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. <br>Last updated: September 22, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-62VMHHREC5"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-62VMHHREC5');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>